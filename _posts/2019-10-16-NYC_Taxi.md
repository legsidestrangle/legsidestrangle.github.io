---
layout: post
title:  "Predicting Demand for Taxi in New York City"
author: "Pawan Bharadwaj"
---

Getting a taxi in a city is always a headache but passengers have alternatives but taxi drivers survive on picking up passengers so it's more important for them to get rides so in this project we try to predict for the taxi drivers the demand based on area such that taxi drivers can chose to go to the nearest area where there is maximum demand which will increase their chances to get a ride. 

Yellow Taxi are famous NYC yellow taxis that provide transportation exclusively through street-hails. The number of taxicabs is limited by a finite number of medallions issued by the TLC. You access this mode of transportation by standing in the street and hailing an available taxi with your hand. The pickups are not pre-arranged.

# Data exploration and cleaning : 

We get the data from : http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml (2015-2016 data) The data used in the attached datasets were collected and provided to the NYC Taxi and Limousine Commission (TLC) 

For this project we will be using only 2015 data. 

<img width="848" alt="Screen Shot 2019-12-06 at 12 09 57 AM" src="https://user-images.githubusercontent.com/16144527/70263612-c8e59100-17bc-11ea-9830-e148d94ab530.png">

Next we will be doing univariate analysis and removing outlier/illegitimate values which may be caused due to some error

Lets define the lattitude and logitude of NYC and remove any outliers first we check for pick up 

It is inferred from the source https://www.flickr.com/places/info/2459115 that New York is bounded by the location cordinates(lat,long) - (40.5774, -74.15) & (40.9176,-73.7004) so hence any cordinates not within these cordinates are not considered by us as we are only concerned with pickups which originate within New York.

<img width="952" alt="Screen Shot 2019-12-06 at 12 13 19 AM" src="https://user-images.githubusercontent.com/16144527/70263828-3c879e00-17bd-11ea-98e0-17dd1012ebdc.png">

<img width="633" alt="Screen Shot 2019-12-06 at 12 14 27 AM" src="https://user-images.githubusercontent.com/16144527/70263921-65a82e80-17bd-11ea-98ba-40af87f48c81.png">

As we see there are some points which are not in NYC and we have to remove that since we are only concerned with NYC. 

Now we do the same for drop off locations so we will be using the similar code and we find similar observations we found for pick up locations. 

<img width="836" alt="Screen Shot 2019-12-06 at 12 17 13 AM" src="https://user-images.githubusercontent.com/16144527/70264138-dd765900-17bd-11ea-8cbf-0215e57bc043.png">

Next the timestamps are converted to unix in our data we have time in the formate "YYYY-MM-DD HH:MM:SS" we convert this string to python time formate and then into unix time stamp. According to NYC Taxi & Limousine Commision Regulations the maximum allowed trip duration in a 24 hour interval is 12 hours.

We return a data frame which contains the columns
<img width="901" alt="Screen Shot 2019-12-06 at 12 32 06 AM" src="https://user-images.githubusercontent.com/16144527/70265193-e6682a00-17bf-11ea-9406-8c2c2bdab336.png">

We then check the average speed of cab in NYC as we need to know how much the driver can travel during 10 mins interval which will decide how far a driver can go to the place where there is demand. This will determine what area should the driver chose for maximum chance of getting a ride. 

<img width="953" alt="Screen Shot 2019-12-06 at 12 37 31 AM" src="https://user-images.githubusercontent.com/16144527/70265625-cf760780-17c0-11ea-9963-c1693663169b.png">

We find that average speed of NYC cab is 12.45 miles/hour which means the driver can travel 2 miles in a 10 mins interval. We are trying to find the demand of taxis every 10 mins hence this is important for us to know. 
We also do the same with respect to time and fare since the code involed is similar i will be jumping to univariate analys where we remove the outliers and have the remaining date for cleaning. 

<img width="625" alt="Screen Shot 2019-12-06 at 12 43 09 AM" src="https://user-images.githubusercontent.com/16144527/70265921-6773f100-17c1-11ea-8e80-6ce22ac758fb.png">

<img width="786" alt="Screen Shot 2019-12-06 at 12 43 45 AM" src="https://user-images.githubusercontent.com/16144527/70265955-7e1a4800-17c1-11ea-935d-32a29c3f6a69.png">


# Clustering

Once we have our date next step is to cluster the area we are interested in, we check for optimum clusterning by using kmeans 
<img width="998" alt="Screen Shot 2019-12-06 at 12 49 30 AM" src="https://user-images.githubusercontent.com/16144527/70266351-51b2fb80-17c2-11ea-8155-d6807647fd0a.png">
<img width="584" alt="Screen Shot 2019-12-06 at 12 49 40 AM" src="https://user-images.githubusercontent.com/16144527/70266354-52e42880-17c2-11ea-85d8-41cc432f23e7.png">

If check for the 50 clusters you can observe that there are two clusters with only 0.3 miles apart from each other
so we choose 40 clusters for solve the further problem and plot the clustering profile here. 

<img width="713" alt="Screen Shot 2019-12-06 at 12 52 14 AM" src="https://user-images.githubusercontent.com/16144527/70266538-aeaeb180-17c2-11ea-9418-186d73014416.png">


# Modeling

In this we will be using three different models 
Now we get into modelling in order to forecast the pickup densities for the months of Jan, Feb and March of 2016 for which we are using multiple models with two variations

1) Using Ratios of the 2016 data to the 2015 data i.e 𝑅𝑡=𝑃2016𝑡/𝑃2015𝑡

2) Using Previous known values of the 2016 data itself to predict the future values

## Simple Moving Averages

The First Model used is the Moving Averages Model which uses the previous n values in order to predict the next value
Using Ratio Values - 𝑅𝑡=(𝑅𝑡−1+𝑅𝑡−2+𝑅𝑡−3....𝑅𝑡−𝑛)/𝑛
<img width="788" alt="Screen Shot 2019-12-06 at 1 00 32 AM" src="https://user-images.githubusercontent.com/16144527/70267152-df431b00-17c3-11ea-83d9-6ee29d8adf2e.png">

Next we use the Moving averages of the 2016 values itself to predict the future value using 𝑃𝑡=(𝑃𝑡−1+𝑃𝑡−2+𝑃𝑡−3....𝑃𝑡−𝑛)/𝑛
<img width="699" alt="Screen Shot 2019-12-06 at 1 00 41 AM" src="https://user-images.githubusercontent.com/16144527/70267157-e0744800-17c3-11ea-913c-bc101a1608d2.png">

## Weighted Moving Averages

The Moving Avergaes Model used gave equal importance to all the values in the window used, but we know intuitively that the future is more likely to be similar to the latest values and less similar to the older values. Weighted Averages converts this analogy into a mathematical relationship giving the highest weight while computing the averages to the latest previous value and decreasing weights to the subsequent older ones

Weighted Moving Averages using Ratio Values - 𝑅𝑡=(𝑁∗𝑅𝑡−1+(𝑁−1)∗𝑅𝑡−2+(𝑁−2)∗𝑅𝑡−3....1∗𝑅𝑡−𝑛)/(𝑁∗(𝑁+1)/2)


<img width="680" alt="Screen Shot 2019-12-06 at 1 02 17 AM" src="https://user-images.githubusercontent.com/16144527/70267267-1a454e80-17c4-11ea-8d6a-ee151bd8a4a1.png">

Weighted Moving Averages using Previous 2016 Values - 𝑃𝑡=(𝑁∗𝑃𝑡−1+(𝑁−1)∗𝑃𝑡−2+(𝑁−2)∗𝑃𝑡−3....1∗𝑃𝑡−𝑛)/(𝑁∗(𝑁+1)/2)

<img width="793" alt="Screen Shot 2019-12-06 at 1 02 03 AM" src="https://user-images.githubusercontent.com/16144527/70267262-17e2f480-17c4-11ea-84d0-a937eb908220.png">

## Exponential Weighted Moving Averages

Through weighted averaged we have satisfied the analogy of giving higher weights to the latest value and decreasing weights to the subsequent ones but we still do not know which is the correct weighting scheme as there are infinetly many possibilities in which we can assign weights in a non-increasing order and tune the the hyperparameter window-size. To simplify this process we use Exponential Moving Averages which is a more logical way towards assigning weights and at the same time also using an optimal window-size.

In exponential moving averages we use a single hyperparameter alpha (𝛼)
which is a value between 0 & 1 and based on the value of the hyperparameter alpha the weights and the window sizes are configured.
For eg. If 𝛼=0.9 then the number of days on which the value of the current iteration is based is~1/(1−𝛼)=10 i.e. we consider values 10 days prior before we predict the value for the current iteration. Also the weights are assigned using 2/(𝑁+1)=0.18
,where N = number of prior values being considered, hence from this it is implied that the first or latest value is assigned a weight of 0.18 which keeps exponentially decreasing for the subsequent values.

𝑅′𝑡=𝛼∗𝑅𝑡−1+(1−𝛼)∗𝑅′𝑡−1

<img width="799" alt="Screen Shot 2019-12-06 at 1 04 56 AM" src="https://user-images.githubusercontent.com/16144527/70267444-790ac800-17c4-11ea-98c7-6b149c7f77ba.png">

For 2016  values : 

<img width="683" alt="Screen Shot 2019-12-06 at 1 05 03 AM" src="https://user-images.githubusercontent.com/16144527/70267450-7b6d2200-17c4-11ea-82c8-c84fd58e57f4.png">

## Comparing the baseline models 
We have chosen our error metric for comparison between models as MAPE (Mean Absolute Percentage Error) so that we can know that on an average how good is our model with predictions and MSE (Mean Squared Error) is also used so that we have a clearer understanding as to how well our forecasting model performs with outliers so that we make sure that there is not much of a error margin between our prediction and the actual value
<img width="740" alt="Screen Shot 2019-12-06 at 1 07 01 AM" src="https://user-images.githubusercontent.com/16144527/70267587-c1c28100-17c4-11ea-898f-9d16f7894205.png">

From the above matrix it is inferred that the best forecasting model for our prediction would be:- 𝑃′𝑡=𝛼∗𝑃𝑡−1+(1−𝛼)∗𝑃′𝑡−1 i.e Exponential Moving Averages using 2016 Values

















