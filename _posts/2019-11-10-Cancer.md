---
layout: post
title: "Cancer Diagnosis based on Genetic Data"
author: "Pawan Bharadwaj"
---


In this problem we will be trying to automate the classification of evidence based on each genetic variations.
Once sequenced, a cancer tumor can have thousands of genetic mutations. But the challenge is distinguishing the mutations that contribute to tumor growth (drivers) from the neutral mutations (passengers). So in general the following steps are followed 
 1) A molecular pathologist selects a list of genetic variations of interest that he/she want to analyze
 2) The molecular pathologist searches for evidence in the medical literature that somehow are relevant to the genetic variations of interest
 3) Finally this molecular pathologist spends a huge amount of time analyzing the evidence related to each of the variations to classify them
 
We collect the data from kaggle and the problem is  Multi class classification problem as there are 9 different classes labels  for us to classify. 

# Exploratory Data Analysis 

The data we will be dealing with are of two types one conatins the information about the genetic mutations and the other contains the clinical evidence (text) that  human experts/pathologists use to classify the genetic mutations.

<img width="614" alt="Screen Shot 2019-12-20 at 8 26 49 PM" src="https://user-images.githubusercontent.com/16144527/71263085-366df180-2367-11ea-8ab7-c5d5dd425d30.png">
<img width="485" alt="Screen Shot 2019-12-20 at 8 27 01 PM" src="https://user-images.githubusercontent.com/16144527/71263090-3837b500-2367-11ea-8545-1edf0d2a6ec2.png">

The above two data sets are merged after removing those data points where text description is not available. We have to remove them as they don't give us any information. 

<img width="492" alt="Screen Shot 2019-12-20 at 8 51 03 PM" src="https://user-images.githubusercontent.com/16144527/71264666-974af900-236a-11ea-9ac6-bddb652e1adb.png">

We first dviide the date set into three types one being Training set, Validaion set and Test set. The goal of cross-validation is to define a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like overfitting,underfitting and get an insight on how the model will generalize to an independent data set. 
The distribution of the said data sets are shown below. 

<img width="342" alt="Screen Shot 2019-12-20 at 9 21 01 PM" src="https://user-images.githubusercontent.com/16144527/71266557-af247c00-236e-11ea-965f-2b8b2e39f536.png"> <img width="352" alt="Screen Shot 2019-12-20 at 9 21 11 PM" src="https://user-images.githubusercontent.com/16144527/71266559-b0ee3f80-236e-11ea-98e0-e7b5ef57fdaa.png">
<img width="353" alt="Screen Shot 2019-12-20 at 9 21 15 PM" src="https://user-images.githubusercontent.com/16144527/71266563-b21f6c80-236e-11ea-9518-1d139c020dbf.png">

In a 'Random' Model, we generate the NINE class probabilites randomly such that they sum to 1. 


<img width="671" alt="Screen Shot 2019-12-20 at 9 38 02 PM" src="https://user-images.githubusercontent.com/16144527/71267659-2eb34a80-2371-11ea-9289-9be027c6a2ff.png">
<img width="824" alt="Screen Shot 2019-12-20 at 9 38 33 PM" src="https://user-images.githubusercontent.com/16144527/71267664-307d0e00-2371-11ea-8be4-cf0e8d4e4f76.png">
<img width="834" alt="Screen Shot 2019-12-20 at 9 38 46 PM" src="https://user-images.githubusercontent.com/16144527/71267666-31ae3b00-2371-11ea-8dcd-b54ab54063e5.png">
<img width="852" alt="Screen Shot 2019-12-20 at 9 38 54 PM" src="https://user-images.githubusercontent.com/16144527/71267669-32df6800-2371-11ea-80df-f222fe9b66f4.png">

We will be checking the data behaviour on 2 Machine Learning Models and find out which one is the best one for this data set. 
In both cases we will be doing the hyperparameter tuning and then testing the model it on the data after finding out thebest parameters based on the tuning. 

# Naive Bayes Classifier 
A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. This is based on the Bayes theorem. The results are diplayed below, this is after hyper parameter tuning. 
<img width="740" alt="Screen Shot 2019-12-20 at 9 44 39 PM" src="https://user-images.githubusercontent.com/16144527/71268176-5951d300-2372-11ea-8d76-b9a03634ad56.png">
<img width="828" alt="Screen Shot 2019-12-20 at 9 44 50 PM" src="https://user-images.githubusercontent.com/16144527/71268183-5bb42d00-2372-11ea-815d-5f56897d2cb6.png">
<img width="845" alt="Screen Shot 2019-12-20 at 9 45 01 PM" src="https://user-images.githubusercontent.com/16144527/71268187-5d7df080-2372-11ea-8ab2-48f0ba3948a1.png">
<img width="843" alt="Screen Shot 2019-12-20 at 9 45 08 PM" src="https://user-images.githubusercontent.com/16144527/71268191-5f47b400-2372-11ea-9d80-456e44fb359e.png">

#  K Nearest Neighbour Classification¶
An algorithm which is based on the similarity. The results after finding the best K and hyperparameter tuning is given below. 
<img width="623" alt="Screen Shot 2019-12-20 at 9 49 52 PM" src="https://user-images.githubusercontent.com/16144527/71268545-3aa00c00-2373-11ea-9d99-8cb1124f7608.png">
<img width="829" alt="Screen Shot 2019-12-20 at 9 50 02 PM" src="https://user-images.githubusercontent.com/16144527/71268549-3c69cf80-2373-11ea-8d92-0669fffc9d1e.png">
<img width="833" alt="Screen Shot 2019-12-20 at 9 50 14 PM" src="https://user-images.githubusercontent.com/16144527/71268552-3d9afc80-2373-11ea-8b12-94fe264a55b5.png">
<img width="864" alt="Screen Shot 2019-12-20 at 9 50 19 PM" src="https://user-images.githubusercontent.com/16144527/71268553-3f64c000-2373-11ea-85d5-be0743a1573d.png">

# Conclusion
When we compare the log loss and no on mis classfied points of both these models, It's clear that K nearest is much prefered model in this case. We can further investigate with different machine learning models but for this blog we restrict oursleves with these two models. 
The K nearest neighbours doesn't work all the time since there is lot of mismatch in it's similarity matrix and ends up being misclassified a lot espeically when there are more than 3 class labels. 






